{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35833af0-55e1-4bc0-9eda-ede42ae26944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import h5py \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35355c5b-601b-4937-bc9a-88b2c271f7ca",
   "metadata": {},
   "source": [
    "Diabetic retinopathy, a complication associated with diabetes, affects the eys and can lead to blindness if not diagnosed. This condition rsults from damage to the blood vessels inside the retina. Diabetic retinopathy is a leading cause of blindness among adults.\n",
    "\n",
    "The goal of this project is to implement a machine learning model that can accurately predict the presence of diabetic retinopathy using retinal images.We will be using the [diabetic retinopathy](https://www.kaggle.com/datasets/tanlikesmath/diabetic-retinopathy-resized/) dataset from kaggle, consisting of over 35,000 1024x1024 retinal scans. Below is an example of an individual retinal scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3b1f8-ba44-4599-a85a-dd1094ea0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(path):\n",
    "    img = Image.open(path)\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "plot_image('./dataset/resized_train_cropped/34035_right.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24d51f-6b7c-4472-a507-a5eb05b2d68b",
   "metadata": {},
   "source": [
    "We will begin by preprocessing all images in the dataset. Normalization involves converting the .jpeg file into a $m \\times n$ dimensional array. The `normalize()` function converts the elements of the array into 16 bit floating point variables between the value of 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7273e94c-ab32-4985-8adc-838238628931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(path):\n",
    "    \n",
    "    \"\"\" Normalize an image by resizing and scaling pixel values \"\"\"\n",
    "    \n",
    "    # open image and ensure size is 1024x1024\n",
    "    img = Image.open(path).resize((1024, 1024))\n",
    "    # normalize as 16 bit float between 0 and 1\n",
    "    array = np.array(img).astype(np.float16) / 255.0\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae365ee2-1ecc-4524-ae61-3f0ec13217f4",
   "metadata": {},
   "source": [
    "The function below uses a `ImageDataGenerator` to augment a single image per call to `augment()`. The goal of augmentation is to artificially increase diversity in a training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289aaf0d-3c5d-4288-8d3e-1f65d49890fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator for augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range = 20,          # rotate by n degrees\n",
    "    width_shift_range = 0.2,      # horizontal shift\n",
    "    height_shift_range = 0.2,     # vertical shift\n",
    "    shear_range = 0.2,            # shear transform\n",
    "    zoom_range = 0.2,             # zoom in or out\n",
    "    horizontal_flip = True,       # flip horizontally?\n",
    "    fill_mode = 'nearest'         # fill method for new pixels\n",
    ")\n",
    "\n",
    "def augment(image, datagen):\n",
    "    \n",
    "    \"\"\" Augment image using above ImageDataGenerator \"\"\"\n",
    "    \n",
    "    # add dimension for fit\n",
    "    img = np.expand_dims(image, 0)\n",
    "\n",
    "    # create generator\n",
    "    it = datagen.flow(img, batch_size = 1)\n",
    "    \n",
    "    # augment one image\n",
    "    augmented = next(it)[0].astype(np.float16)\n",
    "    \n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579db435-1d4e-4d8e-9170-976534d128b8",
   "metadata": {},
   "source": [
    "The `process_image()` function just calls the `augment()` and `normalize()` functions and returns an exception if anything is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35328554-3198-4eb5-bc55-9d60003763b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(path):\n",
    "    \n",
    "    \"\"\" Process singular image given path \"\"\"\n",
    "    \n",
    "    # try to preprocess\n",
    "    try:\n",
    "        array = normalize(path)\n",
    "        array = augment(array, datagen)\n",
    "        \n",
    "        return array\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f28841c",
   "metadata": {},
   "source": [
    "Instead of having to preprocess the dataset everytime the notebook is run, the `create_cache()` function checks if a preprocessed version of the dataset has been made already. If not, it creates a cache file. The code is written to be as parallel as possible to ensure preprocessing is quick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b23be2d-55a2-44f8-acfb-c12759515815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cache(images_directory, cache_path, batch_size=100):\n",
    "    \n",
    "    \"\"\" Create preprocessed cache \"\"\"\n",
    "    if not os.path.exists(cache_path):\n",
    "        # list all JPEG images in directory\n",
    "        image_paths = [os.path.join(images_directory, f) for f in os.listdir(images_directory) if f.endswith('.jpeg')]\n",
    "        \n",
    "        # Init HDF5 file and create thread pool\n",
    "        with h5py.File(cache_path, 'w') as h5file, ThreadPoolExecutor() as executor:  \n",
    "            for i in range(0, len(image_paths), batch_size):\n",
    "                # get batch of image paths\n",
    "                batch_paths = image_paths[i:i+batch_size]  \n",
    "                # process images in parallel\n",
    "                batch_cache = list(executor.map(process_image, batch_paths))  \n",
    "\n",
    "                # add dataset in HDF5 file from batch\n",
    "                h5file.create_dataset(f'batch_{i//batch_size}', data=batch_cache)\n",
    "                print(f'\\rProcessed {i + len(batch_cache)} out of {len(image_paths)}', end='')  \n",
    "\n",
    "        print(\"\\nCache file created.\")  \n",
    "    else:\n",
    "        print(\"Cache file already exists.\")\n",
    "        \n",
    "create_cache('./dataset/resized_train_cropped/', './dataset/preprocess.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b0ad4-61be-4843-88de-af6142b8ea3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
